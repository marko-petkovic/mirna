{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merkelmauer/mirna/blob/main/Initial_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D7JeCpd2Nep",
        "outputId": "cf927200-5233-483f-be1f-57b815d95b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MgMR4QspjvRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgKU5wNzDK4F",
        "outputId": "fa5ee909-4cf9-40ed-cd5f-3eb8ebee911c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/master')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "import torch.distributions as dist\n",
        "from torchinfo import summary\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "#from model_diva import DIVA\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HuLsYxyh6_ZM"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Classes"
      ],
      "metadata": {
        "id": "axFkNf0cjx2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ae7NZhZGj7Zi"
      },
      "outputs": [],
      "source": [
        "class diva_args:\n",
        "\n",
        "    def __init__(self, zd_dim=32, zx_dim=32, zy_dim=32, d_dim=45, x_dim=7500, \n",
        "                 y_dim=2, aux_loss_multiplier_y=3500, aux_loss_multiplier_d=2000,\n",
        "                 beta_d=1, beta_x=1, beta_y=1):\n",
        "\n",
        "        self.zd_dim = zd_dim\n",
        "        self.zx_dim = zx_dim\n",
        "        self.zy_dim = zy_dim\n",
        "        self.d_dim = d_dim\n",
        "        self.x_dim = x_dim\n",
        "        self.y_dim = y_dim\n",
        "        self.aux_loss_multiplier_y = aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = aux_loss_multiplier_d\n",
        "        self.beta_d = beta_d\n",
        "        self.beta_x = beta_x\n",
        "        self.beta_y = beta_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "tb1vH-a1j7Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "D6ouvuZX3WPs"
      },
      "outputs": [],
      "source": [
        "class MicroRNADataset(Dataset):\n",
        "\n",
        "    def __init__(self, cat_data=True):\n",
        "        \n",
        "        # loading images\n",
        "        \n",
        "        if not cat_data:\n",
        "            print('Loading and Converting Images! (~90min)')\n",
        "            images = np.load('/content/drive/MyDrive/master/modmirbase_train_images.npz')['arr_0']/255\n",
        "            self.images = self.im2col(images)\n",
        "        else:\n",
        "            print('Loading Converted Images (~1min)')\n",
        "            self.images = np.load('/content/drive/MyDrive/master/modmirbase_train_images_wbrgby.npz')#['arr_0']\n",
        "        \n",
        "        \n",
        "        # loading labels\n",
        "        print('Loading Labels! (~10s)')     \n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        labels = np.load('/content/drive/MyDrive/master/modmirbase_train_labels.npz')['arr_0']\n",
        "        self.labels = ohe.fit_transform(labels)\n",
        "        \n",
        "        \n",
        "        # loading names\n",
        "        print('Loading Names! (~5s)')\n",
        "        names =  np.load('/content/drive/MyDrive/master/modmirbase_train_names.npz')['arr_0']\n",
        "        names = [i.decode('utf-8') for i in names]\n",
        "        self.species = ['mmu', 'prd', 'hsa', 'ptr', 'efu', 'cbn', 'gma', 'pma',\n",
        "                        'cel', 'gga', 'ipu', 'ptc', 'mdo', 'cgr', 'bta', 'cin', \n",
        "                        'ppy', 'ssc', 'ath', 'cfa', 'osa', 'mtr', 'gra', 'mml',\n",
        "                        'stu', 'bdi', 'rno', 'oan', 'dre', 'aca', 'eca', 'chi',\n",
        "                        'bmo', 'ggo', 'aly', 'dps', 'mdm', 'ame', 'ppc', 'ssa',\n",
        "                        'ppt', 'tca', 'dme', 'sbi']\n",
        "        # assigning a species label to each observation from species\n",
        "        # with more than 200 observations from past research\n",
        "        self.names = []\n",
        "        for i in names:\n",
        "            append = False\n",
        "            for j in self.species:\n",
        "                if j in i.lower():\n",
        "                    self.names.append(j)\n",
        "                    append = True\n",
        "                    break\n",
        "            if not append:\n",
        "                if 'random' in i.lower() or i.isdigit():\n",
        "                    self.names.append('hsa')\n",
        "                else:\n",
        "                    self.names.append('notfound')\n",
        "        \n",
        "        # performing one hot encoding\n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        self.names_ohe = ohe.fit_transform(np.array(self.names).reshape(-1,1))\n",
        "      \n",
        "    def __len__(self):\n",
        "        return(self.images.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d = self.names_ohe[idx]\n",
        "        y = self.labels[idx]\n",
        "        x = self.images[idx]\n",
        "        x = np.transpose(x, (2,0,1))\n",
        "        return (x, y, d)\n",
        "\n",
        "    def im2col(self, images):\n",
        "        \"\"\"\n",
        "        One hot encodes images\n",
        "        0: black\n",
        "        1: white\n",
        "        2: red\n",
        "        3: green\n",
        "        4: blue\n",
        "        5: yellow\n",
        "        \"\"\"\n",
        "        out_shape = (images.shape[0],images.shape[1],images.shape[2],6)\n",
        "        out_array = np.zeros(out_shape)\n",
        "        for i in range(out_array.shape[0]):\n",
        "            print(f'at image {i} out of {out_array.shape[0]}!')\n",
        "            for j in range(out_array.shape[1]):\n",
        "                for k in range(out_array.shape[2]):\n",
        "                    #print(images[i,j,k].shape, images[i,j,k])\n",
        "                    curr = images[i,j,k]\n",
        "                    if (curr==np.array([0,0,0])).all():\n",
        "                        out_array[i,j,k,0] = 1\n",
        "                    elif (curr==np.array([1,1,1])).all():\n",
        "                        out_array[i,j,k,1] = 1\n",
        "                    elif (curr==np.array([1,0,0])).all():\n",
        "                        out_array[i,j,k,2] = 1\n",
        "                    elif (curr==np.array([0,1,0])).all():\n",
        "                        out_array[i,j,k,3] = 1\n",
        "                    elif (curr==np.array([0,0,1])).all():\n",
        "                        out_array[i,j,k,4] = 1\n",
        "                    elif (curr==np.array([1,1,0])).all():\n",
        "                        out_array[i,j,k,5] = 1\n",
        "                    else:\n",
        "                        print(\"big error!!!\")\n",
        "        print('saving new file!')\n",
        "        with open('/content/drive/MyDrive/master/modmirbase_train_images_wbrgby.npz', 'wb') as f:\n",
        "            np.save(f, out_array)\n",
        "        return out_array\n",
        "                    \n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder classes"
      ],
      "metadata": {
        "id": "Xxj-WGXMj-Ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RKizJuchX9uG"
      },
      "outputs": [],
      "source": [
        "# Decoders\n",
        "class px(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(px, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(zd_dim + zx_dim + zy_dim, 2048, bias=False), \n",
        "                                 nn.BatchNorm1d(2048), \n",
        "                                 nn.ReLU())\n",
        "        self.up1 = nn.Upsample(scale_factor=3)\n",
        "        self.de1 = nn.Sequential(nn.ConvTranspose2d(512, 256, kernel_size=5, stride=1, padding=2, bias=False), \n",
        "                                 nn.BatchNorm2d(256),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Conv2d(256,256, kernel_size=3, padding='same', stride=1),\n",
        "                                 nn.BatchNorm2d(256),\n",
        "                                 nn.ReLU()\n",
        "                                 )\n",
        "        self.up2 = nn.Upsample(scale_factor=3)\n",
        "        self.de2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=5, stride=1, padding=2, bias=False), \n",
        "                                 nn.BatchNorm2d(128), \n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Conv2d(128,128, kernel_size=3, padding='same', stride=1),\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 nn.ReLU())\n",
        "        self.up3 = nn.Upsample(scale_factor=3)\n",
        "        self.de3 = nn.Sequential(nn.ConvTranspose2d(128, 128, kernel_size=5, stride=1, padding=2, bias=False), \n",
        "                                 nn.BatchNorm2d(128), \n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Conv2d(128,128, kernel_size=3, padding='same', stride=1),\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 nn.ReLU())\n",
        "        self.de4 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, padding='same', stride=1),\n",
        "                                 nn.BatchNorm2d(64),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Conv2d(64, 6, kernel_size=1, padding='same'),\n",
        "                                 nn.Softmax(dim=1))\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.de1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.de2[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.de3[0].weight)\n",
        "        self.de4[0].bias.data.zero_()\n",
        "        self.de4[3].bias.data.zero_()\n",
        "\n",
        "    def forward(self, zd, zx, zy):\n",
        "        if zx is None:\n",
        "            zdzxzy = torch.cat((zd, zy), dim=-1)\n",
        "        else:\n",
        "            zdzxzy = torch.cat((zd, zx, zy), dim=-1)\n",
        "        h = self.fc1(zdzxzy)\n",
        "        h = h.view(-1, 512, 1, 4)\n",
        "        h = self.up1(h)\n",
        "        h = self.de1(h)\n",
        "        h = self.up2(h)\n",
        "        h = self.de2(h)\n",
        "        h = self.up3(h)\n",
        "        h = self.de3(h)[:,:,1:-1, 4:-4]\n",
        "        loc_img = self.de4(h)\n",
        "\n",
        "        return loc_img\n",
        "\n",
        "\n",
        "class pzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzd, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(d_dim, zd_dim, bias=False), \n",
        "                                 nn.BatchNorm1d(zd_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zd_dim, zd_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zd_dim, zd_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, d):\n",
        "        hidden = self.fc1(d)\n",
        "        zd_loc = self.fc21(hidden)\n",
        "        zd_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class pzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzy, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(y_dim, zy_dim, bias=False),\n",
        "                                 nn.BatchNorm1d(zy_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zy_dim, zy_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zy_dim, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, y):\n",
        "        hidden = self.fc1(y)\n",
        "        zy_loc = self.fc21(hidden)\n",
        "        zy_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zy_loc, zy_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1y8G2S1zxzTH"
      },
      "outputs": [],
      "source": [
        "# pzy_ = pzy(45, 7500, 2, 32,32,32)\n",
        "# summary(pzy_, (1,2))\n",
        "#pzy_ = px(45, 7500, 2, 32,32,32)\n",
        "#summary(pzy_, [(1,32),(1,32),(1,32)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Endcoder Classes"
      ],
      "metadata": {
        "id": "YmNnZWXvkCDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "78ZFH8gYl_-z"
      },
      "outputs": [],
      "source": [
        "class qzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzd, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(6, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4608, zd_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4608, zd_dim), nn.Softplus())\n",
        "        \n",
        "        \n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[7].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[10].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[14].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[17].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4608)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class qzx(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzx, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(6, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4608, zx_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4608, zx_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[7].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[10].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[14].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[17].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4608)\n",
        "        zx_loc = self.fc11(h)\n",
        "        zx_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zx_loc, zx_scale\n",
        "\n",
        "\n",
        "class qzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzy, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # block 1\n",
        "            nn.Conv2d(6, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(32), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # block 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(64), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # block 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, bias=False, padding='same'), \n",
        "            nn.BatchNorm2d(128), \n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4608, zy_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4608, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[7].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[10].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[14].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[17].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4608)\n",
        "        zy_loc = self.fc11(h)\n",
        "        zy_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zy_loc, zy_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary predictors classes"
      ],
      "metadata": {
        "id": "601pquQEkE6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XhIOunSHcc9b"
      },
      "outputs": [],
      "source": [
        "# Auxiliary tasks\n",
        "class qd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qd, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zd_dim, d_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zd):\n",
        "        h = F.relu(zd)\n",
        "        loc_d = self.fc1(h)\n",
        "\n",
        "        return loc_d\n",
        "\n",
        "\n",
        "class qy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qy, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zy_dim, y_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zy):\n",
        "        h = F.relu(zy)\n",
        "        loc_y = self.fc1(h)\n",
        "\n",
        "        return loc_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full model class"
      ],
      "metadata": {
        "id": "vn_gJdNSkH_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BgR5BnQN1WWG"
      },
      "outputs": [],
      "source": [
        "class DIVA(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(DIVA, self).__init__()\n",
        "        self.zd_dim = args.zd_dim\n",
        "        self.zx_dim = args.zx_dim\n",
        "        self.zy_dim = args.zy_dim\n",
        "        self.d_dim = args.d_dim\n",
        "        self.x_dim = args.x_dim\n",
        "        self.y_dim = args.y_dim\n",
        "\n",
        "        self.start_zx = self.zd_dim\n",
        "        self.start_zy = self.zd_dim + self.zx_dim\n",
        "\n",
        "        self.px = px(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzd = pzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzy = pzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qzd = qzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        if self.zx_dim != 0:\n",
        "            self.qzx = qzx(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qzy = qzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qd = qd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qy = qy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.aux_loss_multiplier_y = args.aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = args.aux_loss_multiplier_d\n",
        "\n",
        "        self.beta_d = args.beta_d\n",
        "        self.beta_x = args.beta_x\n",
        "        self.beta_y = args.beta_y\n",
        "\n",
        "        self.cuda()\n",
        "\n",
        "    def forward(self, d, x, y):\n",
        "        # Encode\n",
        "        zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "        if self.zx_dim != 0:\n",
        "            zx_q_loc, zx_q_scale = self.qzx(x)\n",
        "        zy_q_loc, zy_q_scale = self.qzy(x)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        qzd = dist.Normal(zd_q_loc, zd_q_scale)\n",
        "        zd_q = qzd.rsample()\n",
        "        if self.zx_dim != 0:\n",
        "            qzx = dist.Normal(zx_q_loc, zx_q_scale)\n",
        "            zx_q = qzx.rsample()\n",
        "        else:\n",
        "            qzx = None\n",
        "            zx_q = None\n",
        "\n",
        "        qzy = dist.Normal(zy_q_loc, zy_q_scale)\n",
        "        zy_q = qzy.rsample()\n",
        "\n",
        "        # Decode\n",
        "        x_recon = self.px(zd_q, zx_q, zy_q)\n",
        "\n",
        "        zd_p_loc, zd_p_scale = self.pzd(d)\n",
        "\n",
        "        if self.zx_dim != 0:\n",
        "            zx_p_loc, zx_p_scale = torch.zeros(zd_p_loc.size()[0], self.zx_dim).cuda(),\\\n",
        "                                   torch.ones(zd_p_loc.size()[0], self.zx_dim).cuda()\n",
        "        zy_p_loc, zy_p_scale = self.pzy(y)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        pzd = dist.Normal(zd_p_loc, zd_p_scale)\n",
        "        if self.zx_dim != 0:\n",
        "            pzx = dist.Normal(zx_p_loc, zx_p_scale)\n",
        "        else:\n",
        "            pzx = None\n",
        "        pzy = dist.Normal(zy_p_loc, zy_p_scale)\n",
        "\n",
        "        # Auxiliary losses\n",
        "        d_hat = self.qd(zd_q)\n",
        "        y_hat = self.qy(zy_q)\n",
        "\n",
        "        return x_recon, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q\n",
        "\n",
        "    def loss_function(self, d, x, y=None):\n",
        "        if y is None:  # unsupervised\n",
        "            # Do standard forward pass for everything not involving y\n",
        "            zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "            if self.zx_dim != 0:\n",
        "                zx_q_loc, zx_q_scale = self.qzx(x)\n",
        "            zy_q_loc, zy_q_scale = self.qzy(x)\n",
        "\n",
        "            qzd = dist.Normal(zd_q_loc, zd_q_scale)\n",
        "            zd_q = qzd.rsample()\n",
        "            if self.zx_dim != 0:\n",
        "                qzx = dist.Normal(zx_q_loc, zx_q_scale)\n",
        "                zx_q = qzx.rsample()\n",
        "            else:\n",
        "                zx_q = None\n",
        "            qzy = dist.Normal(zy_q_loc, zy_q_scale)\n",
        "            zy_q = qzy.rsample()\n",
        "\n",
        "            zd_p_loc, zd_p_scale = self.pzd(d)\n",
        "            if self.zx_dim != 0:\n",
        "                zx_p_loc, zx_p_scale = torch.zeros(zd_p_loc.size()[0], self.zx_dim).cuda(), \\\n",
        "                                       torch.ones(zd_p_loc.size()[0], self.zx_dim).cuda()\n",
        "\n",
        "            pzd = dist.Normal(zd_p_loc, zd_p_scale)\n",
        "\n",
        "            if self.zx_dim != 0:\n",
        "                pzx = dist.Normal(zx_p_loc, zx_p_scale)\n",
        "            else:\n",
        "                pzx = None\n",
        "\n",
        "            d_hat = self.qd(zd_q)\n",
        "\n",
        "            x_recon = self.px(zd_q, zx_q, zy_q)\n",
        "\n",
        "            x_recon = x_recon.view(-1, 256)\n",
        "            x_target = (x.view(-1) * 255).long()\n",
        "            CE_x = F.cross_entropy(x_recon, x_target, reduction='sum')\n",
        "\n",
        "            zd_p_minus_zd_q = torch.sum(pzd.log_prob(zd_q) - qzd.log_prob(zd_q))\n",
        "            if self.zx_dim != 0:\n",
        "                KL_zx = torch.sum(pzx.log_prob(zx_q) - qzx.log_prob(zx_q))\n",
        "            else:\n",
        "                KL_zx = 0\n",
        "\n",
        "            _, d_target = d.max(dim=1)\n",
        "            CE_d = F.cross_entropy(d_hat, d_target, reduction='sum')\n",
        "\n",
        "\n",
        "            # Create labels and repeats of zy_q and qzy\n",
        "            y_onehot = torch.eye(10)\n",
        "            y_onehot = y_onehot.repeat(1, 100)\n",
        "            y_onehot = y_onehot.view(1000, 10).cuda()\n",
        "\n",
        "            zy_q = zy_q.repeat(10, 1)\n",
        "            zy_q_loc, zy_q_scale = zy_q_loc.repeat(10, 1), zy_q_scale.repeat(10, 1)\n",
        "            qzy = dist.Normal(zy_q_loc, zy_q_scale)\n",
        "\n",
        "            # Do forward pass for everything involving y\n",
        "            zy_p_loc, zy_p_scale = self.pzy(y_onehot)\n",
        "\n",
        "            # Reparameterization trick\n",
        "            pzy = dist.Normal(zy_p_loc, zy_p_scale)\n",
        "\n",
        "            # Auxiliary losses\n",
        "            y_hat = self.qy(zy_q)\n",
        "\n",
        "            # Marginals\n",
        "            alpha_y = F.softmax(y_hat, dim=-1)\n",
        "            qy = dist.OneHotCategorical(alpha_y)\n",
        "            prob_qy = torch.exp(qy.log_prob(y_onehot))\n",
        "\n",
        "            zy_p_minus_zy_q = torch.sum(pzy.log_prob(zy_q) - qzy.log_prob(zy_q), dim=-1)\n",
        "\n",
        "            marginal_zy_p_minus_zy_q = torch.sum(prob_qy * zy_p_minus_zy_q)\n",
        "\n",
        "            prior_y = torch.tensor(1/10).cuda()\n",
        "            prior_y_minus_qy = torch.log(prior_y) - qy.log_prob(y_onehot)\n",
        "            marginal_prior_y_minus_qy = torch.sum(prob_qy * prior_y_minus_qy)\n",
        "\n",
        "            return CE_x \\\n",
        "                   - self.beta_d * zd_p_minus_zd_q \\\n",
        "                   - self.beta_x * KL_zx \\\n",
        "                   - self.beta_y * marginal_zy_p_minus_zy_q \\\n",
        "                   - marginal_prior_y_minus_qy \\\n",
        "                   + self.aux_loss_multiplier_d * CE_d\n",
        "\n",
        "        else: # supervised\n",
        "            x_recon, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q = self.forward(d, x, y)\n",
        "            #print(x_recon.shape, x.shape)\n",
        "            #x_recon = x_recon.view(-1, 256)\n",
        "            #x_target = (x.view(-1) * 255).long()\n",
        "\n",
        "            #CE_x = -log_mix_dep_Logistic_256(x, x_recon, average=False, n_comps=10)\n",
        "            x_recon = x_recon.view(-1,6)\n",
        "            x_target = x.view(-1,6)\n",
        "            CE_x = F.cross_entropy(x_recon, x_target, reduction='sum')\n",
        "            \n",
        "            zd_p_minus_zd_q = torch.sum(pzd.log_prob(zd_q) - qzd.log_prob(zd_q))\n",
        "            if self.zx_dim != 0:\n",
        "                KL_zx = torch.sum(pzx.log_prob(zx_q) - qzx.log_prob(zx_q))\n",
        "            else:\n",
        "                KL_zx = 0\n",
        "\n",
        "            zy_p_minus_zy_q = torch.sum(pzy.log_prob(zy_q) - qzy.log_prob(zy_q))\n",
        "\n",
        "            _, d_target = d.max(dim=1)\n",
        "            CE_d = F.cross_entropy(d_hat, d_target, reduction='sum')\n",
        "\n",
        "            _, y_target = y.max(dim=1)\n",
        "            CE_y = F.cross_entropy(y_hat, y_target, reduction='sum')\n",
        "\n",
        "            return CE_x \\\n",
        "                   - self.beta_d * zd_p_minus_zd_q \\\n",
        "                   - self.beta_x * KL_zx \\\n",
        "                   - self.beta_y * zy_p_minus_zy_q \\\n",
        "                   + self.aux_loss_multiplier_d * CE_d \\\n",
        "                   + self.aux_loss_multiplier_y * CE_y,\\\n",
        "                   CE_y\n",
        "\n",
        "    def classifier(self, x):\n",
        "        \"\"\"\n",
        "        classify an image (or a batch of images)\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :return: a batch of the corresponding class labels (as one-hots)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "            zd = zd_q_loc\n",
        "            alpha = F.softmax(self.qd(zd), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            d = x.new_zeros(alpha.size())\n",
        "            d = d.scatter_(1, ind, 1.0)\n",
        "\n",
        "            zy_q_loc, zy_q_scale = self.qzy.forward(x)\n",
        "            zy = zy_q_loc\n",
        "            alpha = F.softmax(self.qy(zy), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            y = x.new_zeros(alpha.size())\n",
        "            y = y.scatter_(1, ind, 1.0)\n",
        "\n",
        "        return d, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "LdOsLfYJjBBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing model"
      ],
      "metadata": {
        "id": "R_H_mVMUszt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "sJE0HTJE1ayP"
      },
      "outputs": [],
      "source": [
        "default_args = diva_args(beta_d=10, beta_x=20, beta_y=40, \n",
        "                         zd_dim=32, zy_dim=128, zx_dim=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kxIMSeUD1949"
      },
      "outputs": [],
      "source": [
        "diva = DIVA(default_args).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diva.load_state_dict(torch.load('/content/drive/MyDrive/master/diva_v9.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5C6mSJTDLtu",
        "outputId": "5201fa30-5b34-4b22-9ec9-e96cca653e7a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "rH1E5J-ps3GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNA_dataset = MicroRNADataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myflmDPxjV40",
        "outputId": "a90cf5b7-3cdf-49a4-caf4-f6cfb2f8de1a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Converted Images (~1min)\n",
            "Loading Labels! (~10s)\n",
            "Loading Names! (~5s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XZ3JG664vsnE"
      },
      "outputs": [],
      "source": [
        "#summary(diva, [(1,45),(1,6,25,100),(1,2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ],
      "metadata": {
        "id": "YdYaqWvbjN26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eVGq463Y2m20"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(train_loader, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    epoch_class_y_loss = 0\n",
        "\n",
        "    no_batches = 0\n",
        "    pbar = tqdm(enumerate(train_loader), unit=\"batch\", \n",
        "                                     desc=f'Epoch {epoch}')\n",
        "    for batch_idx, (x, y, d) in pbar:\n",
        "        # To device\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        # print(d)\n",
        "        x, y, d = x.to(DEVICE), y.to(DEVICE), d.to(DEVICE)\n",
        "\n",
        "        # if (epoch % 50 == 0) and (batch_idx == 1):\n",
        "        #     save_reconstructions(model, d, x, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, class_y_loss = model.loss_function(d.float(), x.float(), y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix(loss=loss.item()/x.shape[0], \n",
        "                         y_loss = class_y_loss.item()/x.shape[0])\n",
        "        train_loss += loss\n",
        "        epoch_class_y_loss += class_y_loss\n",
        "        no_batches += 1\n",
        "        # print(f'finished batch {no_batches}!')\n",
        "        # if no_batches == 25:\n",
        "        #     break\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    epoch_class_y_loss /= len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, epoch_class_y_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, diva, optimizer, end_epoch, start_epoch=0):\n",
        "    epoch_loss_sup = []\n",
        "    epoch_loss_y = []\n",
        "\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        avg_epoch_losses_sup, avg_epoch_class_y_loss = train_single_epoch(train_loader, diva, optimizer, epoch)\n",
        "        str_loss_sup = avg_epoch_losses_sup\n",
        "        epoch_loss_sup.append(avg_epoch_losses_sup)\n",
        "        epoch_loss_y.append(avg_epoch_class_y_loss)\n",
        "        str_print = \"epoch {}: avg loss {}\".format(epoch, str_loss_sup)\n",
        "        str_print += \", class y loss {}\".format(avg_epoch_class_y_loss)\n",
        "        print(str_print)\n",
        "    \n",
        "    return epoch_loss_sup, epoch_loss_y"
      ],
      "metadata": {
        "id": "npLjVGs0jHYn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "nI4-NzHxjmci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "48B39rFl79Yh"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(RNA_dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J6y2Ek2677z1"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(diva.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m47XoL87oLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3a94e9-c078-433c-ef30-6aa444945b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41: 272batch [05:37,  1.24s/batch, loss=5.13e+3, y_loss=0.00229]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 41: avg loss 5132.8583984375, class y loss 0.012129459530115128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42: 272batch [05:37,  1.24s/batch, loss=5.93e+3, y_loss=0.00923]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 42: avg loss 5109.94189453125, class y loss 0.012702845968306065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43: 272batch [05:37,  1.24s/batch, loss=5.17e+3, y_loss=0.00527]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 43: avg loss 5103.42236328125, class y loss 0.01252656988799572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44: 272batch [05:37,  1.24s/batch, loss=5.04e+3, y_loss=0.000327]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 44: avg loss 5110.21826171875, class y loss 0.013998324051499367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45: 272batch [05:37,  1.24s/batch, loss=5.25e+3, y_loss=0.00466]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 45: avg loss 5071.49609375, class y loss 0.010739232413470745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46: 236batch [04:54,  1.24s/batch, loss=5.14e+3, y_loss=0.00498]"
          ]
        }
      ],
      "source": [
        "lss, eplss = train(train_loader, diva, optimizer, 50, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14_0zzMCtR7Y"
      },
      "outputs": [],
      "source": [
        "torch.save(diva.state_dict(), '/content/drive/MyDrive/master/diva_v9.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "whsgNltzXDhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling from trained model"
      ],
      "metadata": {
        "id": "VfcwhSNIjqIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaSz6PSlII26"
      },
      "outputs": [],
      "source": [
        "def sample(x, mean=False):\n",
        "    \"\"\"\n",
        "    takes sample for each pixel given probabilities\n",
        "    input: reconstructed x (shape: -1,6,25,100)\n",
        "    output: sample from x (shape: -1,25,100,3)\n",
        "    \"\"\"\n",
        "    color_dict = {1: np.array([1,1,1]), # white\n",
        "                  0: np.array([0,0,0]), # black\n",
        "                  2: np.array([1,0,0]), # red\n",
        "                  3: np.array([0,1,0]), # green\n",
        "                  4: np.array([0,0,1]), # blue\n",
        "                  5: np.array([1,1,0])  # yellow\n",
        "                  }\n",
        "    out = np.zeros((x.shape[0],x.shape[2],x.shape[3], 3))\n",
        "    for i in range(x.shape[0]):\n",
        "      for j in range(x.shape[2]):\n",
        "        for k in range(x.shape[3]):\n",
        "          p = x[i,:,j,k]\n",
        "          p = p.astype(np.float64)\n",
        "          p += 1e-7\n",
        "          #print(p)\n",
        "          p /= p.sum()\n",
        "          #print(p, sum(p))\n",
        "          if not mean:\n",
        "            pix = np.random.choice(np.arange(6), p=p)\n",
        "          else:\n",
        "            pix = np.argmax(p)\n",
        "          out[i,j,k] = color_dict[pix]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DMiSSjWMSzh"
      },
      "outputs": [],
      "source": [
        "def decode(x):\n",
        "    \"\"\"\n",
        "    gives image in pixels given x\n",
        "    input: training x (shape: -1,6,25,100)\n",
        "    output:  x (shape: -1,25,100,3)\n",
        "    \"\"\"\n",
        "    color_dict = {1: np.array([1,1,1]), # white\n",
        "              0: np.array([0,0,0]), # black\n",
        "              2: np.array([1,0,0]), # red\n",
        "              3: np.array([0,1,0]), # green\n",
        "              4: np.array([0,0,1]), # blue\n",
        "              5: np.array([1,1,0])  # yellow\n",
        "              }\n",
        "    out = np.zeros((x.shape[0],x.shape[2],x.shape[3], 3))\n",
        "    for i in range(x.shape[0]):\n",
        "      for j in range(x.shape[2]):\n",
        "        for k in range(x.shape[3]):\n",
        "          for l in range(6):\n",
        "            if x[i,l,j,k] == 1:\n",
        "              out[i,j,k] = color_dict[l]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blYW4SMZG4fv"
      },
      "outputs": [],
      "source": [
        "a = next(enumerate(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahb9oyxiHHdY"
      },
      "outputs": [],
      "source": [
        "[h.shape for h in a[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deiEYKxlmphu"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    diva.eval()\n",
        "    d = a[1][2][:9].to(DEVICE).float()\n",
        "    x = a[1][0][:9].to(DEVICE).float()\n",
        "    y = a[1][1][:9].to(DEVICE).float()\n",
        "    x_recon, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q = diva(d,x,y)\n",
        "    sample_x = sample(x_recon.cpu().numpy(), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NWGV4Xd7bn8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(sample_x[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE3qVVFFLaPm"
      },
      "outputs": [],
      "source": [
        "x_true = decode(x)\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(x_true[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yxJBjKXcMLlj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Initial Experiment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}