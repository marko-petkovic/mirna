{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merkelmauer/mirna/blob/main/StampDIVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D7JeCpd2Nep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154f83ca-f13a-4061-ddbc-7a1c7e1f338e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.3\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.43.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "!pip install torchvision\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "link = '/content/drive/MyDrive/master'"
      ],
      "metadata": {
        "id": "UuXEtFCubCjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MgMR4QspjvRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgKU5wNzDK4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e534ad3-4e30-474e-9824-7adbe851b94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "#sys.path.insert(0,'/content/drive/MyDrive/Marko/master')\n",
        "sys.path.insert(0, link)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torchinfo import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "#writer = SummaryWriter()\n",
        "writer = None \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuLsYxyh6_ZM"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Classes"
      ],
      "metadata": {
        "id": "axFkNf0cjx2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae7NZhZGj7Zi"
      },
      "outputs": [],
      "source": [
        "class diva_args:\n",
        "\n",
        "    def __init__(self, zd_dim=32, zx_dim=32, zy_dim=64, d_dim=45, x_dim=7500, \n",
        "                 y_dim=2, aux_loss_multiplier_y=20, aux_loss_multiplier_d=2,\n",
        "                 beta_d=10, beta_x=10, beta_y=40, \n",
        "                 rec_alpha = 1, rec_beta = 1, rec_gamma = 1):\n",
        "\n",
        "        self.zd_dim = zd_dim\n",
        "        self.zx_dim = zx_dim\n",
        "        self.zy_dim = zy_dim\n",
        "        self.d_dim = d_dim\n",
        "        self.x_dim = x_dim\n",
        "        self.y_dim = y_dim\n",
        "        self.aux_loss_multiplier_y = aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = aux_loss_multiplier_d\n",
        "        self.beta_d = beta_d\n",
        "        self.beta_x = beta_x\n",
        "        self.beta_y = beta_y\n",
        "        self.rec_alpha = rec_alpha\n",
        "        self.rec_beta = rec_beta\n",
        "        self.rec_gamma = rec_gamma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "tb1vH-a1j7Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6ouvuZX3WPs"
      },
      "outputs": [],
      "source": [
        "class MicroRNADataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds='train'):\n",
        "        \n",
        "        # loading images\n",
        "        self.images = np.load(f'{link}/modmirbase_{ds}_images.npz')['arr_0']/255\n",
        "        \n",
        "        \n",
        "        # loading labels\n",
        "        print('Loading Labels! (~10s)')     \n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        labels = np.load(f'{link}/modmirbase_{ds}_labels.npz')['arr_0']\n",
        "        self.labels = ohe.fit_transform(labels)\n",
        "        \n",
        "        \n",
        "        # loading names\n",
        "        print('Loading Names! (~5s)')\n",
        "        names =  np.load(f'{link}/modmirbase_{ds}_names.npz')['arr_0']\n",
        "        names = [i.decode('utf-8') for i in names]\n",
        "        self.species = ['mmu', 'prd', 'hsa', 'ptr', 'efu', 'cbn', 'gma', 'pma',\n",
        "                        'cel', 'gga', 'ipu', 'ptc', 'mdo', 'cgr', 'bta', 'cin', \n",
        "                        'ppy', 'ssc', 'ath', 'cfa', 'osa', 'mtr', 'gra', 'mml',\n",
        "                        'stu', 'bdi', 'rno', 'oan', 'dre', 'aca', 'eca', 'chi',\n",
        "                        'bmo', 'ggo', 'aly', 'dps', 'mdm', 'ame', 'ppc', 'ssa',\n",
        "                        'ppt', 'tca', 'dme', 'sbi']\n",
        "        # assigning a species label to each observation from species\n",
        "        # with more than 200 observations from past research\n",
        "        self.names = []\n",
        "        for i in names:\n",
        "            append = False\n",
        "            for j in self.species:\n",
        "                if j in i.lower():\n",
        "                    self.names.append(j)\n",
        "                    append = True\n",
        "                    break\n",
        "            if not append:\n",
        "                if 'random' in i.lower() or i.isdigit():\n",
        "                    self.names.append('hsa')\n",
        "                else:\n",
        "                    self.names.append('notfound')\n",
        "        \n",
        "        # performing one hot encoding\n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        self.names_ohe = ohe.fit_transform(np.array(self.names).reshape(-1,1))\n",
        "      \n",
        "    def __len__(self):\n",
        "        return(self.images.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d = self.names_ohe[idx]\n",
        "        y = self.labels[idx]\n",
        "        x = self.images[idx]\n",
        "        x = np.transpose(x, (2,0,1))\n",
        "        return (x, y, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder classes"
      ],
      "metadata": {
        "id": "Xxj-WGXMj-Ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKizJuchX9uG"
      },
      "outputs": [],
      "source": [
        "# Decoders\n",
        "class px(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(px, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(zd_dim + zx_dim + zy_dim, 200, bias=False),  \n",
        "                                 nn.ReLU())\n",
        "        \n",
        "        # Predicting length and color of each bar\n",
        "        self.up1 = nn.Upsample(scale_factor=5)\n",
        "        self.de1 = nn.Sequential(nn.ConvTranspose1d(5,25,kernel_size = 5,\n",
        "                                                    stride = 1, padding = 2,\n",
        "                                                    bias=False),\n",
        "                                 nn.ReLU()\n",
        "                                 )\n",
        "        # Predicting color of each bar\n",
        "        self.color_bar = nn.Sequential(nn.Conv1d(25,5, kernel_size = 3, padding = 'same'),\n",
        "                                      nn.Softmax(dim=1))\n",
        "        \n",
        "        # Predicting the length of each bar\n",
        "        self.length_bar = nn.Sequential(nn.Conv1d(25, 13, kernel_size = 3, padding = 'same'),\n",
        "                                        nn.Softmax(dim=1))\n",
        "\n",
        "        # Predicting length of the RNA strand\n",
        "        self.length_RNA = nn.Sequential(nn.Linear(200,100), nn.Softmax())\n",
        "        \n",
        "    def forward(self, zd, zx, zy):\n",
        "        if zx is None:\n",
        "            zdzxzy = torch.cat((zd, zy), dim=-1)\n",
        "        else:\n",
        "            zdzxzy = torch.cat((zd, zx, zy), dim=-1)\n",
        "        h = self.fc1(zdzxzy)\n",
        "        len_RNA = self.length_RNA(h)\n",
        "        \n",
        "        h = h.view(-1, 5, 40)\n",
        "        h = self.up1(h)\n",
        "        h = self.de1(h)\n",
        "        \n",
        "        len_bar = self.length_bar(h)\n",
        "        col_bar = self.color_bar(h)\n",
        "        \n",
        "        return len_RNA, len_bar, col_bar\n",
        "\n",
        "    def reconstruct_image(self, len_RNA, len_bar, col_bar, sample=False):\n",
        "        \"\"\"\n",
        "        reconstructs RNA image given output from decoder\n",
        "        even indexes of len_bar and col_bar   -> top\n",
        "        uneven indexes of len_bar and col_bar -> bottom\n",
        "        function does not support sampling yet\n",
        "        color reconstructions: 0: black\n",
        "                               1: red\n",
        "                               2: blue\n",
        "                               3: green\n",
        "                               4: yellow\n",
        "        \"\"\"\n",
        "        color_dict = {\n",
        "                  0: np.array([0,0,0]), # black\n",
        "                  1: np.array([1,0,0]), # red\n",
        "                  3: np.array([0,1,0]), # green\n",
        "                  2: np.array([0,0,1]), # blue\n",
        "                  4: np.array([1,1,0])  # yellow\n",
        "                  }\n",
        "    \n",
        "        \n",
        "        len_RNA = len_RNA.cpu().numpy()#.reshape((100,))\n",
        "        len_bar = len_bar.cpu().numpy()\n",
        "        col_bar = col_bar.cpu().numpy()\n",
        "        n = len_RNA.shape[0]\n",
        "        output = np.ones((n,25,100,3))\n",
        "\n",
        "        for i in range(n):\n",
        "            if sample:\n",
        "                limit = np.random.choice(np.arange(100), p = len_RNA[i])\n",
        "            else:\n",
        "                limit = np.argmax(len_RNA[i])\n",
        "\n",
        "            for j in range(limit+1):\n",
        "                if sample:\n",
        "                    _len_bar_1 = np.random.choice(np.arange(1,14), p = len_bar[i, :,2*j]) \n",
        "                    _len_bar_2 = np.random.choice(np.arange(1,14), p = len_bar[i, :, 2*j+1])\n",
        "                    _col_bar_1 = np.random.choice(np.arange(5), p = col_bar[i, :, 2*j])\n",
        "                    _col_bar_2 = np.random.choice(np.arange(5), p = col_bar[i,:, 2*j+1])\n",
        "                else:\n",
        "                    _len_bar_1 = np.argmax(len_bar[i,:, 2*j]) + 1 \n",
        "                    _len_bar_2 = np.argmax(len_bar[i,:, 2*j + 1]) + 1\n",
        "                    _col_bar_1 = np.argmax(col_bar[i,:, 2*j])\n",
        "                    _col_bar_2 = np.argmax(col_bar[i,:, 2*j+1])\n",
        "                \n",
        "                h1 = 13-_len_bar_1\n",
        "                # paint upper bar\n",
        "                output[i, h1:13, j] = color_dict[_col_bar_1]\n",
        "        \n",
        "                # paint lower bar\n",
        "                output[i, 13:13+_len_bar_2, j] = color_dict[_col_bar_2]\n",
        "        \n",
        "        \n",
        "        return output\n",
        "\n",
        "class pzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzd, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(d_dim, zd_dim, bias=False), \n",
        "                                 nn.BatchNorm1d(zd_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zd_dim, zd_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zd_dim, zd_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, d):\n",
        "        hidden = self.fc1(d)\n",
        "        zd_loc = self.fc21(hidden)\n",
        "        zd_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class pzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzy, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(y_dim, zy_dim, bias=False),\n",
        "                                 nn.BatchNorm1d(zy_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zy_dim, zy_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zy_dim, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, y):\n",
        "        hidden = self.fc1(y)\n",
        "        zy_loc = self.fc21(hidden)\n",
        "        zy_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zy_loc, zy_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y8G2S1zxzTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b81f354-1bc3-4b8d-9f79-e85751eda813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "px                                       --                        --\n",
              "├─Sequential: 1-1                        [1, 200]                  --\n",
              "│    └─Linear: 2-1                       [1, 200]                  19,200\n",
              "│    └─ReLU: 2-2                         [1, 200]                  --\n",
              "├─Sequential: 1-2                        [1, 100]                  --\n",
              "│    └─Linear: 2-3                       [1, 100]                  20,100\n",
              "│    └─Softmax: 2-4                      [1, 100]                  --\n",
              "├─Upsample: 1-3                          [1, 5, 200]               --\n",
              "├─Sequential: 1-4                        [1, 25, 200]              --\n",
              "│    └─ConvTranspose1d: 2-5              [1, 25, 200]              625\n",
              "│    └─ReLU: 2-6                         [1, 25, 200]              --\n",
              "├─Sequential: 1-5                        [1, 13, 200]              --\n",
              "│    └─Conv1d: 2-7                       [1, 13, 200]              988\n",
              "│    └─Softmax: 2-8                      [1, 13, 200]              --\n",
              "├─Sequential: 1-6                        [1, 5, 200]               --\n",
              "│    └─Conv1d: 2-9                       [1, 5, 200]               380\n",
              "│    └─Softmax: 2-10                     [1, 5, 200]               --\n",
              "==========================================================================================\n",
              "Total params: 41,293\n",
              "Trainable params: 41,293\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.44\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.07\n",
              "Params size (MB): 0.17\n",
              "Estimated Total Size (MB): 0.24\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# pzy_ = pzy(45, 7500, 2, 32,32,32)\n",
        "# summary(pzy_, (1,2))\n",
        "pzy_ = px(45, 7500, 2, 32,32,32)\n",
        "summary(pzy_, [(1,32),(1,32),(1,32)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Endcoder Classes"
      ],
      "metadata": {
        "id": "YmNnZWXvkCDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pzy_.reconstruct_image(torch.zeros((1,100)), torch.zeros((1,13,200)), torch.zeros(1,5,200)).shape"
      ],
      "metadata": {
        "id": "tt82wvITwg4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ZFH8gYl_-z"
      },
      "outputs": [],
      "source": [
        "class qzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzd, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zd_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zd_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class qzx(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzx, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zx_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zx_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class qzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzy, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zy_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary predictors classes"
      ],
      "metadata": {
        "id": "601pquQEkE6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhIOunSHcc9b"
      },
      "outputs": [],
      "source": [
        "# Auxiliary tasks\n",
        "class qd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qd, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zd_dim, d_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zd):\n",
        "        h = F.relu(zd)\n",
        "        loc_d = self.fc1(h)\n",
        "\n",
        "        return loc_d\n",
        "\n",
        "\n",
        "class qy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qy, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zy_dim, y_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zy):\n",
        "        h = F.relu(zy)\n",
        "        loc_y = self.fc1(h)\n",
        "\n",
        "        return loc_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full model class"
      ],
      "metadata": {
        "id": "vn_gJdNSkH_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgR5BnQN1WWG"
      },
      "outputs": [],
      "source": [
        "class StampDIVA(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(StampDIVA, self).__init__()\n",
        "        self.zd_dim = args.zd_dim\n",
        "        self.zx_dim = args.zx_dim\n",
        "        self.zy_dim = args.zy_dim\n",
        "        self.d_dim = args.d_dim\n",
        "        self.x_dim = args.x_dim\n",
        "        self.y_dim = args.y_dim\n",
        "\n",
        "        self.start_zx = self.zd_dim\n",
        "        self.start_zy = self.zd_dim + self.zx_dim\n",
        "\n",
        "        self.px = px(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzd = pzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzy = pzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qzd = qzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        if self.zx_dim != 0:\n",
        "            self.qzx = qzx(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qzy = qzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qd = qd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qy = qy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.aux_loss_multiplier_y = args.aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = args.aux_loss_multiplier_d\n",
        "\n",
        "        self.beta_d = args.beta_d\n",
        "        self.beta_x = args.beta_x\n",
        "        self.beta_y = args.beta_y\n",
        "\n",
        "        self.rec_alpha = args.rec_alpha\n",
        "        self.rec_beta = args.rec_beta\n",
        "        self.rec_gamma = args.rec_gamma\n",
        "\n",
        "        self.cuda()\n",
        "\n",
        "    def forward(self, d, x, y):\n",
        "        # Encode\n",
        "        zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "        if self.zx_dim != 0:\n",
        "            zx_q_loc, zx_q_scale = self.qzx(x)\n",
        "        zy_q_loc, zy_q_scale = self.qzy(x)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        qzd = dist.Normal(zd_q_loc, zd_q_scale)\n",
        "        zd_q = qzd.rsample()\n",
        "        if self.zx_dim != 0:\n",
        "            qzx = dist.Normal(zx_q_loc, zx_q_scale)\n",
        "            zx_q = qzx.rsample()\n",
        "        else:\n",
        "            qzx = None\n",
        "            zx_q = None\n",
        "\n",
        "        qzy = dist.Normal(zy_q_loc, zy_q_scale)\n",
        "        zy_q = qzy.rsample()\n",
        "\n",
        "        # Decode\n",
        "        x_len, x_bar, x_col = self.px(zd_q, zx_q, zy_q)\n",
        "\n",
        "        zd_p_loc, zd_p_scale = self.pzd(d)\n",
        "\n",
        "        if self.zx_dim != 0:\n",
        "            zx_p_loc, zx_p_scale = torch.zeros(zd_p_loc.size()[0], self.zx_dim).cuda(),\\\n",
        "                                   torch.ones(zd_p_loc.size()[0], self.zx_dim).cuda()\n",
        "        zy_p_loc, zy_p_scale = self.pzy(y)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        pzd = dist.Normal(zd_p_loc, zd_p_scale)\n",
        "        if self.zx_dim != 0:\n",
        "            pzx = dist.Normal(zx_p_loc, zx_p_scale)\n",
        "        else:\n",
        "            pzx = None\n",
        "        pzy = dist.Normal(zy_p_loc, zy_p_scale)\n",
        "\n",
        "        # Auxiliary losses\n",
        "        d_hat = self.qd(zd_q)\n",
        "        y_hat = self.qy(zy_q)\n",
        "\n",
        "        return x_len, x_bar, x_col, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q\n",
        "\n",
        "    def loss_function(self, d, x, y):\n",
        "          x_len, x_bar, x_col, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q = self.forward(d, x, y)\n",
        "          \n",
        "          out_len, out_bar, out_col = self.get_encoded_values(x)\n",
        "          \n",
        "          CE_len = F.cross_entropy(x_len, out_len, reduction='sum')\n",
        "          CE_bar = F.cross_entropy(x_bar, out_bar, reduction='sum')\n",
        "          CE_col = F.cross_entropy(x_col, out_col, reduction='sum')\n",
        "\n",
        "          zd_p_minus_zd_q = torch.sum(pzd.log_prob(zd_q) - qzd.log_prob(zd_q))\n",
        "          if self.zx_dim != 0:\n",
        "              KL_zx = torch.sum(pzx.log_prob(zx_q) - qzx.log_prob(zx_q))\n",
        "          else:\n",
        "              KL_zx = 0\n",
        "\n",
        "          zy_p_minus_zy_q = torch.sum(pzy.log_prob(zy_q) - qzy.log_prob(zy_q))\n",
        "\n",
        "          _, d_target = d.max(dim=1)\n",
        "          CE_d = F.cross_entropy(d_hat, d_target, reduction='sum')\n",
        "\n",
        "          _, y_target = y.max(dim=1)\n",
        "          CE_y = F.cross_entropy(y_hat, y_target, reduction='sum')\n",
        "\n",
        "          return self.rec_alpha * CE_len \\\n",
        "                  + self.rec_beta * CE_bar \\\n",
        "                  + self.rec_gamma * CE_col \\\n",
        "                  - self.beta_d * zd_p_minus_zd_q \\\n",
        "                  - self.beta_x * KL_zx \\\n",
        "                  - self.beta_y * zy_p_minus_zy_q \\\n",
        "                  + self.aux_loss_multiplier_d * CE_d \\\n",
        "                  + self.aux_loss_multiplier_y * CE_y,\\\n",
        "                  CE_y\n",
        "\n",
        "    def get_encoded_values(self, x):\n",
        "        \"\"\"\n",
        "        given an image or batch of images\n",
        "        returns length of strand, length of bars and colors of bars\n",
        "        \"\"\"\n",
        "        n = x.shape[0]\n",
        "        out_len = torch.zeros((n,100)).to(DEVICE)\n",
        "        out_col = torch.zeros((n,5,200)).to(DEVICE)\n",
        "        out_bar = torch.zeros((n,13,200)).to(DEVICE)\n",
        "\n",
        "        for i in range(n):\n",
        "            rna_len = 0\n",
        "            for j in range(100):\n",
        "                if (x[i,:,12,j] == torch.tensor([1,1,1]).to(DEVICE)).all():\n",
        "                   out_len[i,rna_len-1] = 1\n",
        "                   break\n",
        "                else:\n",
        "                    rna_len += 1\n",
        "                    # check color of bars\n",
        "                    out_col[i, self.get_color(x[i,:,12,j]) ,2*j] = 1 \n",
        "                    out_col[i, self.get_color(x[i,:,13,j]), 2*j+1] = 1\n",
        "                    # check length of bars\n",
        "                    len1 = 0\n",
        "                    # loop until white pixel\n",
        "                    while not (x[i,:,12-len1,j] == torch.tensor([1.,1.,1.]).to(DEVICE)).all():\n",
        "                        len1 += 1\n",
        "                        if 13-len1 == 0:\n",
        "                           break\n",
        "                    out_bar[i, len1-1, 2*j] = 1\n",
        "\n",
        "                    len2 = 0\n",
        "                    while not (x[i,:,13+len2,j] == torch.tensor([1.,1.,1.]).to(DEVICE)).all():\n",
        "                        len2 += 1\n",
        "                        if 13+len2 == 25:\n",
        "                            break\n",
        "                    out_bar[i, len2-1, 2*j+1] = 1\n",
        "        return out_len, out_bar, out_col\n",
        "\n",
        "    def get_color(self, pixel):\n",
        "        \"\"\"\n",
        "        returns the encoded value for a pixel\n",
        "        \"\"\"\n",
        "        if (pixel == torch.tensor([0,0,0]).to(DEVICE)).all():  \n",
        "            return 0 # black\n",
        "        elif (pixel == torch.tensor([1,0,0]).to(DEVICE)).all():  \n",
        "            return 1 # red\n",
        "        elif (pixel == torch.tensor([0,0,1]).to(DEVICE)).all():  \n",
        "            return 2 # blue\n",
        "        elif (pixel == torch.tensor([0,1,0]).to(DEVICE)).all():  \n",
        "            return 3 # green\n",
        "        elif (pixel == torch.tensor([1,1,0]).to(DEVICE)).all():  \n",
        "            return 4 # yellow\n",
        "        else:\n",
        "            print(\"Something wrong!\")\n",
        "        \n",
        "                  \n",
        "\n",
        "    def classifier(self, x):\n",
        "        \"\"\"\n",
        "        classify an image (or a batch of images)\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :return: a batch of the corresponding class labels (as one-hots)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "            zd = zd_q_loc\n",
        "            alpha = F.softmax(self.qd(zd), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            d = x.new_zeros(alpha.size())\n",
        "            d = d.scatter_(1, ind, 1.0)\n",
        "\n",
        "            zy_q_loc, zy_q_scale = self.qzy.forward(x)\n",
        "            zy = zy_q_loc\n",
        "            alpha = F.softmax(self.qy(zy), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            y = x.new_zeros(alpha.size())\n",
        "            y = y.scatter_(1, ind, 1.0)\n",
        "\n",
        "        return d, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "LdOsLfYJjBBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing model"
      ],
      "metadata": {
        "id": "R_H_mVMUszt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJE0HTJE1ayP"
      },
      "outputs": [],
      "source": [
        "default_args = diva_args(zd_dim=32, zx_dim=64, zy_dim=32,\n",
        "                         aux_loss_multiplier_y=50, aux_loss_multiplier_d=2,\n",
        "                         rec_alpha = 1, rec_beta = 20, rec_gamma = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxIMSeUD1949"
      },
      "outputs": [],
      "source": [
        "diva = StampDIVA(default_args).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(diva, [(1,45),(1,3,25,100),(1,2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ZiBzfeZwro",
        "outputId": "0bf9daec-2d2c-45b8-bbc1-4aca23080726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "StampDIVA                                --                        --\n",
              "├─qzd: 1-1                               [1, 32]                   --\n",
              "│    └─Sequential: 2-1                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-1                  [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-2                    [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-3               [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-5                    [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-6               [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-2                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-7                  [1, 32]                   135,200\n",
              "│    └─Sequential: 2-3                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-8                  [1, 32]                   135,200\n",
              "│    │    └─Softplus: 3-9                [1, 32]                   --\n",
              "├─qzx: 1-2                               [1, 64]                   --\n",
              "│    └─Sequential: 2-4                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-10                 [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-11                   [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-12              [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-13                 [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-14                   [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-15              [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-5                   [1, 64]                   --\n",
              "│    │    └─Linear: 3-16                 [1, 64]                   270,400\n",
              "│    └─Sequential: 2-6                   [1, 64]                   --\n",
              "│    │    └─Linear: 3-17                 [1, 64]                   270,400\n",
              "│    │    └─Softplus: 3-18               [1, 64]                   --\n",
              "├─qzy: 1-3                               [1, 32]                   --\n",
              "│    └─Sequential: 2-7                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-19                 [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-20                   [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-21              [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-22                 [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-23                   [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-24              [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-8                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-25                 [1, 32]                   135,200\n",
              "│    └─Sequential: 2-9                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-26                 [1, 32]                   135,200\n",
              "│    │    └─Softplus: 3-27               [1, 32]                   --\n",
              "├─px: 1-4                                [1, 100]                  --\n",
              "│    └─Sequential: 2-10                  [1, 200]                  --\n",
              "│    │    └─Linear: 3-28                 [1, 200]                  25,600\n",
              "│    │    └─ReLU: 3-29                   [1, 200]                  --\n",
              "│    └─Sequential: 2-11                  [1, 100]                  --\n",
              "│    │    └─Linear: 3-30                 [1, 100]                  20,100\n",
              "│    │    └─Softmax: 3-31                [1, 100]                  --\n",
              "│    └─Upsample: 2-12                    [1, 5, 200]               --\n",
              "│    └─Sequential: 2-13                  [1, 25, 200]              --\n",
              "│    │    └─ConvTranspose1d: 3-32        [1, 25, 200]              625\n",
              "│    │    └─ReLU: 3-33                   [1, 25, 200]              --\n",
              "│    └─Sequential: 2-14                  [1, 13, 200]              --\n",
              "│    │    └─Conv1d: 3-34                 [1, 13, 200]              988\n",
              "│    │    └─Softmax: 3-35                [1, 13, 200]              --\n",
              "│    └─Sequential: 2-15                  [1, 5, 200]               --\n",
              "│    │    └─Conv1d: 3-36                 [1, 5, 200]               380\n",
              "│    │    └─Softmax: 3-37                [1, 5, 200]               --\n",
              "├─pzd: 1-5                               [1, 32]                   --\n",
              "│    └─Sequential: 2-16                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-38                 [1, 32]                   1,440\n",
              "│    │    └─BatchNorm1d: 3-39            [1, 32]                   64\n",
              "│    │    └─ReLU: 3-40                   [1, 32]                   --\n",
              "│    └─Sequential: 2-17                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-41                 [1, 32]                   1,056\n",
              "│    └─Sequential: 2-18                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-42                 [1, 32]                   1,056\n",
              "│    │    └─Softplus: 3-43               [1, 32]                   --\n",
              "├─pzy: 1-6                               [1, 32]                   --\n",
              "│    └─Sequential: 2-19                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-44                 [1, 32]                   64\n",
              "│    │    └─BatchNorm1d: 3-45            [1, 32]                   64\n",
              "│    │    └─ReLU: 3-46                   [1, 32]                   --\n",
              "│    └─Sequential: 2-20                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-47                 [1, 32]                   1,056\n",
              "│    └─Sequential: 2-21                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-48                 [1, 32]                   1,056\n",
              "│    │    └─Softplus: 3-49               [1, 32]                   --\n",
              "├─qd: 1-7                                [1, 45]                   --\n",
              "│    └─Linear: 2-22                      [1, 45]                   1,485\n",
              "├─qy: 1-8                                [1, 2]                    --\n",
              "│    └─Linear: 2-23                      [1, 2]                    66\n",
              "==========================================================================================\n",
              "Total params: 1,297,500\n",
              "Trainable params: 1,297,500\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 56.60\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2.03\n",
              "Params size (MB): 5.19\n",
              "Estimated Total Size (MB): 7.25\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#diva.load_state_dict(torch.load(f'{link}/stampdiva_v2.0.0.pth'))"
      ],
      "metadata": {
        "id": "s5C6mSJTDLtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "rH1E5J-ps3GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNA_dataset = MicroRNADataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myflmDPxjV40",
        "outputId": "5e7840a6-39dd-4783-d83a-dfa2bb77046e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Labels! (~10s)\n",
            "Loading Names! (~5s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RNA_dataset_test = MicroRNADataset('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut2P5RSaMoDR",
        "outputId": "7b2f567b-16ce-45a1-9b28-81a8b978de0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Labels! (~10s)\n",
            "Loading Names! (~5s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ],
      "metadata": {
        "id": "YdYaqWvbjN26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVGq463Y2m20"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(train_loader, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    epoch_class_y_loss = 0\n",
        "\n",
        "    no_batches = 0\n",
        "    pbar = tqdm(enumerate(train_loader), unit=\"batch\", \n",
        "                                     desc=f'Epoch {epoch}')\n",
        "    for batch_idx, (x, y, d) in pbar:\n",
        "        # To device\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        # print(d)\n",
        "        x, y, d = x.to(DEVICE), y.to(DEVICE), d.to(DEVICE)\n",
        "\n",
        "        # if (epoch % 50 == 0) and (batch_idx == 1):\n",
        "        #     save_reconstructions(model, d, x, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, class_y_loss = model.loss_function(d.float(), x.float(), y.float())\n",
        "        _, y_pred = model.classifier(x.float())\n",
        "        acc = ((y == y_pred).all(axis=1)*1.0).mean().item()\n",
        "        if writer is not None:\n",
        "          writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "          writer.add_scalar(\"y_loss/train\", class_y_loss, epoch)\n",
        "          writer.add_scalar(\"y_acc/train\", acc, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix(loss=loss.item()/x.shape[0], \n",
        "                         y_loss = class_y_loss.item()/x.shape[0])\n",
        "        train_loss += loss\n",
        "        epoch_class_y_loss += class_y_loss\n",
        "        no_batches += 1\n",
        "        # print(f'finished batch {no_batches}!')\n",
        "        # if no_batches == 25:\n",
        "        #     break\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    epoch_class_y_loss /= len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, epoch_class_y_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_epoch(test_loader, model, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    epoch_class_y_loss = 0\n",
        "    test_corr = 0\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x,y,d) in enumerate(test_loader):\n",
        "            x, y, d = x.to(DEVICE), y.to(DEVICE), d.to(DEVICE)\n",
        "            loss, class_y_loss = model.loss_function(d.float(), x.float(), y.float())\n",
        "            _, y_pred = model.classifier(x.float())\n",
        "            test_corr += (y == y_pred).all(axis=1).sum().item()\n",
        "            acc = ((y == y_pred).all(axis=1)*1.).mean().item()\n",
        "            if writer is not None:\n",
        "              writer.add_scalar(\"Loss/test\", loss, epoch)\n",
        "              writer.add_scalar(\"y_loss/test\", class_y_loss, epoch)\n",
        "              writer.add_scalar(\"y_acc/test\", acc, epoch)\n",
        "            test_loss += loss\n",
        "            epoch_class_y_loss += class_y_loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    epoch_class_y_loss /= len(test_loader.dataset)\n",
        "    acc = test_corr/len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, epoch_class_y_loss, acc\n",
        "  "
      ],
      "metadata": {
        "id": "dT7E0C3nM3qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, diva, optimizer, end_epoch, start_epoch=0):\n",
        "    epoch_loss_sup = []\n",
        "    epoch_loss_y = []\n",
        "\n",
        "    y_loss_test = []\n",
        "    test_loss = []\n",
        "    test_acc_lst = []\n",
        "\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        avg_epoch_losses_sup, avg_epoch_class_y_loss = train_single_epoch(train_loader, diva, optimizer, epoch)\n",
        "        str_loss_sup = avg_epoch_losses_sup\n",
        "        epoch_loss_sup.append(avg_epoch_losses_sup)\n",
        "        epoch_loss_y.append(avg_epoch_class_y_loss)\n",
        "        str_print = \"epoch {}: avg train loss {}\".format(epoch, str_loss_sup)\n",
        "        str_print += \", class y train loss {}\".format(avg_epoch_class_y_loss)\n",
        "        print(str_print)\n",
        "\n",
        "        test_lss, epoch_class_y_loss_test, test_acc = test_single_epoch(test_loader, diva, epoch)\n",
        "        test_loss.append(test_lss)\n",
        "        y_loss_test.append(epoch_class_y_loss_test)\n",
        "        test_acc_lst.append(test_acc)\n",
        "        str_print = \"epoch {}: avg test loss {}\".format(epoch, test_lss)\n",
        "        str_print += \", class y test loss {}\".format(epoch_class_y_loss_test)\n",
        "        str_print += \", test accuracy {}\".format(test_acc)\n",
        "        print(str_print)\n",
        "    if writer is not None:    \n",
        "      writer.flush()\n",
        "    return epoch_loss_sup, epoch_loss_y"
      ],
      "metadata": {
        "id": "npLjVGs0jHYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "nI4-NzHxjmci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48B39rFl79Yh"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(RNA_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(RNA_dataset_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6y2Ek2677z1"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(diva.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m47XoL87oLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d1a418-fb85-403e-90e0-0f7c45526f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1: 0batch [00:00, ?batch/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "Epoch 1: 272batch [1:24:49, 18.71s/batch, loss=8.81e+3, y_loss=0.705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: avg train loss 8760.3603515625, class y train loss 0.6922470927238464\n",
            "epoch 1: avg test loss 8476.990234375, class y test loss 0.645126461982727, test accuracy 0.8600900477118473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 272batch [1:23:32, 18.43s/batch, loss=8.71e+3, y_loss=0.679]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2: avg train loss 8460.578125, class y train loss 0.6466320753097534\n"
          ]
        }
      ],
      "source": [
        "lss, eplss = train(train_loader, diva, optimizer, 2, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14_0zzMCtR7Y"
      },
      "outputs": [],
      "source": [
        "torch.save(diva.state_dict(), f'{link}/stampdiva_v3.0.0.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JUqEE7bHGjO"
      },
      "outputs": [],
      "source": [
        "lss, eplss = train(train_loader, diva, optimizer, 6, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYgruvcHGjP"
      },
      "outputs": [],
      "source": [
        "torch.save(diva.state_dict(), f'{link}/stampdiva_v3.0.1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gl6VY6zFHDg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "whsgNltzXDhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling from trained model"
      ],
      "metadata": {
        "id": "VfcwhSNIjqIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blYW4SMZG4fv"
      },
      "outputs": [],
      "source": [
        "a = next(enumerate(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deiEYKxlmphu"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    diva.eval()\n",
        "    d = a[1][2][:9].to(DEVICE).float()\n",
        "    x = a[1][0][:9].to(DEVICE).float()\n",
        "    y = a[1][1][:9].to(DEVICE).float()\n",
        "    x_1, x_2, x_3, _, _, _, _, _, _, _, _, _, _, _ = diva(d,x,y)\n",
        "    out = diva.px.reconstruct_image(x_1, x_2, x_3)\n",
        "    #sample_x = sample(x_recon.cpu().numpy(), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NWGV4Xd7bn8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(out[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE3qVVFFLaPm"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(x[i].cpu().permute(1,2,0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "StampDIVA",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}