{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merkelmauer/mirna/blob/main/StampDIVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4D7JeCpd2Nep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd26f6f-7c8c-496f-ba89-549899937c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.3\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.43.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "!pip install torchvision\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "link = '/content/drive/MyDrive/master'"
      ],
      "metadata": {
        "id": "UuXEtFCubCjx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MgMR4QspjvRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VgKU5wNzDK4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d40ae92-1fb6-4e56-defa-4b073810ff54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "#sys.path.insert(0,'/content/drive/MyDrive/Marko/master')\n",
        "sys.path.insert(0, link)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from torchinfo import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "#writer = SummaryWriter()\n",
        "writer = None \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HuLsYxyh6_ZM"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Classes"
      ],
      "metadata": {
        "id": "axFkNf0cjx2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ae7NZhZGj7Zi"
      },
      "outputs": [],
      "source": [
        "class diva_args:\n",
        "\n",
        "    def __init__(self, zd_dim=32, zx_dim=32, zy_dim=64, d_dim=45, x_dim=7500, \n",
        "                 y_dim=2, aux_loss_multiplier_y=20, aux_loss_multiplier_d=2,\n",
        "                 beta_d=10, beta_x=10, beta_y=40, \n",
        "                 rec_alpha = 1, rec_beta = 1, rec_gamma = 1):\n",
        "\n",
        "        self.zd_dim = zd_dim\n",
        "        self.zx_dim = zx_dim\n",
        "        self.zy_dim = zy_dim\n",
        "        self.d_dim = d_dim\n",
        "        self.x_dim = x_dim\n",
        "        self.y_dim = y_dim\n",
        "        self.aux_loss_multiplier_y = aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = aux_loss_multiplier_d\n",
        "        self.beta_d = beta_d\n",
        "        self.beta_x = beta_x\n",
        "        self.beta_y = beta_y\n",
        "        self.rec_alpha = rec_alpha\n",
        "        self.rec_beta = rec_beta\n",
        "        self.rec_gamma = rec_gamma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "tb1vH-a1j7Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D6ouvuZX3WPs"
      },
      "outputs": [],
      "source": [
        "class MicroRNADataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds='train'):\n",
        "        \n",
        "        # loading images\n",
        "        self.images = np.load(f'{link}/modmirbase_{ds}_images.npz')['arr_0']/255\n",
        "        \n",
        "        \n",
        "        # loading labels\n",
        "        print('Loading Labels! (~10s)')     \n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        labels = np.load(f'{link}/modmirbase_{ds}_labels.npz')['arr_0']\n",
        "        self.labels = ohe.fit_transform(labels)\n",
        "        \n",
        "        \n",
        "        # loading names\n",
        "        print('Loading Names! (~5s)')\n",
        "        names =  np.load(f'{link}/modmirbase_{ds}_names.npz')['arr_0']\n",
        "        names = [i.decode('utf-8') for i in names]\n",
        "        self.species = ['mmu', 'prd', 'hsa', 'ptr', 'efu', 'cbn', 'gma', 'pma',\n",
        "                        'cel', 'gga', 'ipu', 'ptc', 'mdo', 'cgr', 'bta', 'cin', \n",
        "                        'ppy', 'ssc', 'ath', 'cfa', 'osa', 'mtr', 'gra', 'mml',\n",
        "                        'stu', 'bdi', 'rno', 'oan', 'dre', 'aca', 'eca', 'chi',\n",
        "                        'bmo', 'ggo', 'aly', 'dps', 'mdm', 'ame', 'ppc', 'ssa',\n",
        "                        'ppt', 'tca', 'dme', 'sbi']\n",
        "        # assigning a species label to each observation from species\n",
        "        # with more than 200 observations from past research\n",
        "        self.names = []\n",
        "        for i in names:\n",
        "            append = False\n",
        "            for j in self.species:\n",
        "                if j in i.lower():\n",
        "                    self.names.append(j)\n",
        "                    append = True\n",
        "                    break\n",
        "            if not append:\n",
        "                if 'random' in i.lower() or i.isdigit():\n",
        "                    self.names.append('hsa')\n",
        "                else:\n",
        "                    self.names.append('notfound')\n",
        "        \n",
        "        # performing one hot encoding\n",
        "        ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "        self.names_ohe = ohe.fit_transform(np.array(self.names).reshape(-1,1))\n",
        "      \n",
        "    def __len__(self):\n",
        "        return(self.images.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d = self.names_ohe[idx]\n",
        "        y = self.labels[idx]\n",
        "        x = self.images[idx]\n",
        "        x = np.transpose(x, (2,0,1))\n",
        "        return (x, y, d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder classes"
      ],
      "metadata": {
        "id": "Xxj-WGXMj-Ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RKizJuchX9uG"
      },
      "outputs": [],
      "source": [
        "# Decoders\n",
        "class px(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(px, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(zd_dim + zx_dim + zy_dim, 200, bias=False),  \n",
        "                                 nn.ReLU())\n",
        "        \n",
        "        # Predicting length and color of each bar\n",
        "        self.up1 = nn.Upsample(scale_factor=5)\n",
        "        self.de1 = nn.Sequential(nn.ConvTranspose1d(5,25,kernel_size = 5,\n",
        "                                                    stride = 1, padding = 2,\n",
        "                                                    bias=False),\n",
        "                                 nn.ReLU()\n",
        "                                 )\n",
        "        # Predicting color of each bar\n",
        "        self.color_bar = nn.Sequential(nn.Conv1d(25,5, kernel_size = 3, padding = 'same'),\n",
        "                                      nn.Softmax(dim=1))\n",
        "        \n",
        "        # Predicting the length of each bar\n",
        "        self.length_bar = nn.Sequential(nn.Conv1d(25, 13, kernel_size = 3, padding = 'same'),\n",
        "                                        nn.Softmax(dim=1))\n",
        "\n",
        "        # Predicting length of the RNA strand\n",
        "        self.length_RNA = nn.Sequential(nn.Linear(200,100), nn.Softmax())\n",
        "        \n",
        "    def forward(self, zd, zx, zy):\n",
        "        if zx is None:\n",
        "            zdzxzy = torch.cat((zd, zy), dim=-1)\n",
        "        else:\n",
        "            zdzxzy = torch.cat((zd, zx, zy), dim=-1)\n",
        "        h = self.fc1(zdzxzy)\n",
        "        len_RNA = self.length_RNA(h)\n",
        "        \n",
        "        h = h.view(-1, 5, 40)\n",
        "        h = self.up1(h)\n",
        "        h = self.de1(h)\n",
        "        \n",
        "        len_bar = self.length_bar(h)\n",
        "        col_bar = self.color_bar(h)\n",
        "        \n",
        "        return len_RNA, len_bar, col_bar\n",
        "\n",
        "    def reconstruct_image(self, len_RNA, len_bar, col_bar, sample=False):\n",
        "        \"\"\"\n",
        "        reconstructs RNA image given output from decoder\n",
        "        even indexes of len_bar and col_bar   -> top\n",
        "        uneven indexes of len_bar and col_bar -> bottom\n",
        "        function does not support sampling yet\n",
        "        color reconstructions: 0: black\n",
        "                               1: red\n",
        "                               2: blue\n",
        "                               3: green\n",
        "                               4: yellow\n",
        "        \"\"\"\n",
        "        color_dict = {\n",
        "                  0: np.array([0,0,0]), # black\n",
        "                  1: np.array([1,0,0]), # red\n",
        "                  3: np.array([0,1,0]), # green\n",
        "                  2: np.array([0,0,1]), # blue\n",
        "                  4: np.array([1,1,0])  # yellow\n",
        "                  }\n",
        "    \n",
        "        \n",
        "        len_RNA = len_RNA.cpu().numpy()#.reshape((100,))\n",
        "        len_bar = len_bar.cpu().numpy()\n",
        "        col_bar = col_bar.cpu().numpy()\n",
        "        n = len_RNA.shape[0]\n",
        "        output = np.ones((n,25,100,3))\n",
        "\n",
        "        for i in range(n):\n",
        "            if sample:\n",
        "                limit = np.random.choice(np.arange(100), p = len_RNA[i])\n",
        "            else:\n",
        "                limit = np.argmax(len_RNA[i])\n",
        "\n",
        "            for j in range(limit+1):\n",
        "                if sample:\n",
        "                    _len_bar_1 = np.random.choice(np.arange(1,14), p = len_bar[i, :,2*j]) \n",
        "                    _len_bar_2 = np.random.choice(np.arange(1,14), p = len_bar[i, :, 2*j+1])\n",
        "                    _col_bar_1 = np.random.choice(np.arange(5), p = col_bar[i, :, 2*j])\n",
        "                    _col_bar_2 = np.random.choice(np.arange(5), p = col_bar[i,:, 2*j+1])\n",
        "                else:\n",
        "                    _len_bar_1 = np.argmax(len_bar[i,:, 2*j]) + 1 \n",
        "                    _len_bar_2 = np.argmax(len_bar[i,:, 2*j + 1]) + 1\n",
        "                    _col_bar_1 = np.argmax(col_bar[i,:, 2*j])\n",
        "                    _col_bar_2 = np.argmax(col_bar[i,:, 2*j+1])\n",
        "                \n",
        "                h1 = 13-_len_bar_1\n",
        "                # paint upper bar\n",
        "                output[i, h1:13, j] = color_dict[_col_bar_1]\n",
        "        \n",
        "                # paint lower bar\n",
        "                output[i, 13:13+_len_bar_2, j] = color_dict[_col_bar_2]\n",
        "        \n",
        "        \n",
        "        return output\n",
        "\n",
        "class pzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzd, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(d_dim, zd_dim, bias=False), \n",
        "                                 nn.BatchNorm1d(zd_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zd_dim, zd_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zd_dim, zd_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, d):\n",
        "        hidden = self.fc1(d)\n",
        "        zd_loc = self.fc21(hidden)\n",
        "        zd_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class pzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(pzy, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Linear(y_dim, zy_dim, bias=False),\n",
        "                                 nn.BatchNorm1d(zy_dim), \n",
        "                                 nn.ReLU())\n",
        "        self.fc21 = nn.Sequential(nn.Linear(zy_dim, zy_dim))\n",
        "        self.fc22 = nn.Sequential(nn.Linear(zy_dim, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc21[0].weight)\n",
        "        self.fc21[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc22[0].weight)\n",
        "        self.fc22[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, y):\n",
        "        hidden = self.fc1(y)\n",
        "        zy_loc = self.fc21(hidden)\n",
        "        zy_scale = self.fc22(hidden) + 1e-7\n",
        "\n",
        "        return zy_loc, zy_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1y8G2S1zxzTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4f4dbd-c522-470e-c61e-13eef223089c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "px                                       --                        --\n",
              "├─Sequential: 1-1                        [1, 200]                  --\n",
              "│    └─Linear: 2-1                       [1, 200]                  19,200\n",
              "│    └─ReLU: 2-2                         [1, 200]                  --\n",
              "├─Sequential: 1-2                        [1, 100]                  --\n",
              "│    └─Linear: 2-3                       [1, 100]                  20,100\n",
              "│    └─Softmax: 2-4                      [1, 100]                  --\n",
              "├─Upsample: 1-3                          [1, 5, 200]               --\n",
              "├─Sequential: 1-4                        [1, 25, 200]              --\n",
              "│    └─ConvTranspose1d: 2-5              [1, 25, 200]              625\n",
              "│    └─ReLU: 2-6                         [1, 25, 200]              --\n",
              "├─Sequential: 1-5                        [1, 13, 200]              --\n",
              "│    └─Conv1d: 2-7                       [1, 13, 200]              988\n",
              "│    └─Softmax: 2-8                      [1, 13, 200]              --\n",
              "├─Sequential: 1-6                        [1, 5, 200]               --\n",
              "│    └─Conv1d: 2-9                       [1, 5, 200]               380\n",
              "│    └─Softmax: 2-10                     [1, 5, 200]               --\n",
              "==========================================================================================\n",
              "Total params: 41,293\n",
              "Trainable params: 41,293\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.44\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.07\n",
              "Params size (MB): 0.17\n",
              "Estimated Total Size (MB): 0.24\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# pzy_ = pzy(45, 7500, 2, 32,32,32)\n",
        "# summary(pzy_, (1,2))\n",
        "pzy_ = px(45, 7500, 2, 32,32,32)\n",
        "summary(pzy_, [(1,32),(1,32),(1,32)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Endcoder Classes"
      ],
      "metadata": {
        "id": "YmNnZWXvkCDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pzy_.reconstruct_image(torch.zeros((1,100)), torch.zeros((1,13,200)), torch.zeros(1,5,200)).shape"
      ],
      "metadata": {
        "id": "tt82wvITwg4j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "78ZFH8gYl_-z"
      },
      "outputs": [],
      "source": [
        "class qzd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzd, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zd_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zd_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class qzx(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzx, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zx_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zx_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale\n",
        "\n",
        "\n",
        "class qzy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qzy, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, bias=False),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        self.fc11 = nn.Sequential(nn.Linear(4224, zy_dim))\n",
        "        self.fc12 = nn.Sequential(nn.Linear(4224, zy_dim), nn.Softplus())\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[0].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder[3].weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc11[0].weight)\n",
        "        self.fc11[0].bias.data.zero_()\n",
        "        torch.nn.init.xavier_uniform_(self.fc12[0].weight)\n",
        "        self.fc12[0].bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(-1, 4224)\n",
        "        zd_loc = self.fc11(h)\n",
        "        zd_scale = self.fc12(h) + 1e-7\n",
        "\n",
        "        return zd_loc, zd_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary predictors classes"
      ],
      "metadata": {
        "id": "601pquQEkE6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XhIOunSHcc9b"
      },
      "outputs": [],
      "source": [
        "# Auxiliary tasks\n",
        "class qd(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qd, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zd_dim, d_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zd):\n",
        "        h = F.relu(zd)\n",
        "        loc_d = self.fc1(h)\n",
        "\n",
        "        return loc_d\n",
        "\n",
        "\n",
        "class qy(nn.Module):\n",
        "    def __init__(self, d_dim, x_dim, y_dim, zd_dim, zx_dim, zy_dim):\n",
        "        super(qy, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(zy_dim, y_dim)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, zy):\n",
        "        h = F.relu(zy)\n",
        "        loc_y = self.fc1(h)\n",
        "\n",
        "        return loc_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full model class"
      ],
      "metadata": {
        "id": "vn_gJdNSkH_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BgR5BnQN1WWG"
      },
      "outputs": [],
      "source": [
        "class StampDIVA(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(StampDIVA, self).__init__()\n",
        "        self.zd_dim = args.zd_dim\n",
        "        self.zx_dim = args.zx_dim\n",
        "        self.zy_dim = args.zy_dim\n",
        "        self.d_dim = args.d_dim\n",
        "        self.x_dim = args.x_dim\n",
        "        self.y_dim = args.y_dim\n",
        "\n",
        "        self.start_zx = self.zd_dim\n",
        "        self.start_zy = self.zd_dim + self.zx_dim\n",
        "\n",
        "        self.px = px(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzd = pzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.pzy = pzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qzd = qzd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        if self.zx_dim != 0:\n",
        "            self.qzx = qzx(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qzy = qzy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.qd = qd(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "        self.qy = qy(self.d_dim, self.x_dim, self.y_dim, self.zd_dim, self.zx_dim, self.zy_dim)\n",
        "\n",
        "        self.aux_loss_multiplier_y = args.aux_loss_multiplier_y\n",
        "        self.aux_loss_multiplier_d = args.aux_loss_multiplier_d\n",
        "\n",
        "        self.beta_d = args.beta_d\n",
        "        self.beta_x = args.beta_x\n",
        "        self.beta_y = args.beta_y\n",
        "\n",
        "        self.rec_alpha = args.rec_alpha\n",
        "        self.rec_beta = args.rec_beta\n",
        "        self.rec_gamma = args.rec_gamma\n",
        "\n",
        "        self.cuda()\n",
        "\n",
        "    def forward(self, d, x, y):\n",
        "        # Encode\n",
        "        zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "        if self.zx_dim != 0:\n",
        "            zx_q_loc, zx_q_scale = self.qzx(x)\n",
        "        zy_q_loc, zy_q_scale = self.qzy(x)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        qzd = dist.Normal(zd_q_loc, zd_q_scale)\n",
        "        zd_q = qzd.rsample()\n",
        "        if self.zx_dim != 0:\n",
        "            qzx = dist.Normal(zx_q_loc, zx_q_scale)\n",
        "            zx_q = qzx.rsample()\n",
        "        else:\n",
        "            qzx = None\n",
        "            zx_q = None\n",
        "\n",
        "        qzy = dist.Normal(zy_q_loc, zy_q_scale)\n",
        "        zy_q = qzy.rsample()\n",
        "\n",
        "        # Decode\n",
        "        x_len, x_bar, x_col = self.px(zd_q, zx_q, zy_q)\n",
        "\n",
        "        zd_p_loc, zd_p_scale = self.pzd(d)\n",
        "\n",
        "        if self.zx_dim != 0:\n",
        "            zx_p_loc, zx_p_scale = torch.zeros(zd_p_loc.size()[0], self.zx_dim).cuda(),\\\n",
        "                                   torch.ones(zd_p_loc.size()[0], self.zx_dim).cuda()\n",
        "        zy_p_loc, zy_p_scale = self.pzy(y)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        pzd = dist.Normal(zd_p_loc, zd_p_scale)\n",
        "        if self.zx_dim != 0:\n",
        "            pzx = dist.Normal(zx_p_loc, zx_p_scale)\n",
        "        else:\n",
        "            pzx = None\n",
        "        pzy = dist.Normal(zy_p_loc, zy_p_scale)\n",
        "\n",
        "        # Auxiliary losses\n",
        "        d_hat = self.qd(zd_q)\n",
        "        y_hat = self.qy(zy_q)\n",
        "\n",
        "        return x_len, x_bar, x_col, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q\n",
        "\n",
        "    def loss_function(self, d, x, y):\n",
        "          x_len, x_bar, x_col, d_hat, y_hat, qzd, pzd, zd_q, qzx, pzx, zx_q, qzy, pzy, zy_q = self.forward(d, x, y)\n",
        "          \n",
        "          out_len, out_bar, out_col = self.get_encoded_values(x)\n",
        "          \n",
        "          CE_len = F.cross_entropy(x_len, out_len, reduction='sum')\n",
        "          CE_bar = F.cross_entropy(x_bar, out_bar, reduction='sum')\n",
        "          CE_col = F.cross_entropy(x_col, out_col, reduction='sum')\n",
        "\n",
        "          zd_p_minus_zd_q = torch.sum(pzd.log_prob(zd_q) - qzd.log_prob(zd_q))\n",
        "          if self.zx_dim != 0:\n",
        "              KL_zx = torch.sum(pzx.log_prob(zx_q) - qzx.log_prob(zx_q))\n",
        "          else:\n",
        "              KL_zx = 0\n",
        "\n",
        "          zy_p_minus_zy_q = torch.sum(pzy.log_prob(zy_q) - qzy.log_prob(zy_q))\n",
        "\n",
        "          _, d_target = d.max(dim=1)\n",
        "          CE_d = F.cross_entropy(d_hat, d_target, reduction='sum')\n",
        "\n",
        "          _, y_target = y.max(dim=1)\n",
        "          CE_y = F.cross_entropy(y_hat, y_target, reduction='sum')\n",
        "\n",
        "          return self.rec_alpha * CE_len \\\n",
        "                  + self.rec_beta * CE_bar \\\n",
        "                  + self.rec_gamma * CE_col \\\n",
        "                  - self.beta_d * zd_p_minus_zd_q \\\n",
        "                  - self.beta_x * KL_zx \\\n",
        "                  - self.beta_y * zy_p_minus_zy_q \\\n",
        "                  + self.aux_loss_multiplier_d * CE_d \\\n",
        "                  + self.aux_loss_multiplier_y * CE_y,\\\n",
        "                  CE_y\n",
        "\n",
        "    def get_encoded_values(self, x):\n",
        "        \"\"\"\n",
        "        given an image or batch of images\n",
        "        returns length of strand, length of bars and colors of bars\n",
        "        \"\"\"\n",
        "        n = x.shape[0]\n",
        "        out_len = torch.zeros((n,100)).to(DEVICE)\n",
        "        out_col = torch.zeros((n,5,200)).to(DEVICE)\n",
        "        out_bar = torch.zeros((n,13,200)).to(DEVICE)\n",
        "\n",
        "        for i in range(n):\n",
        "            rna_len = 0\n",
        "            for j in range(100):\n",
        "                if (x[i,:,12,j] == torch.tensor([1,1,1]).to(DEVICE)).all():\n",
        "                   out_len[i,rna_len-1] = 1\n",
        "                   break\n",
        "                else:\n",
        "                    rna_len += 1\n",
        "                    # check color of bars\n",
        "                    out_col[i, self.get_color(x[i,:,12,j]) ,2*j] = 1 \n",
        "                    out_col[i, self.get_color(x[i,:,13,j]), 2*j+1] = 1\n",
        "                    # check length of bars\n",
        "                    len1 = 0\n",
        "                    # loop until white pixel\n",
        "                    while not (x[i,:,12-len1,j] == torch.tensor([1.,1.,1.]).to(DEVICE)).all():\n",
        "                        len1 += 1\n",
        "                        if 13-len1 == 0:\n",
        "                           break\n",
        "                    out_bar[i, len1-1, 2*j] = 1\n",
        "\n",
        "                    len2 = 0\n",
        "                    while not (x[i,:,13+len2,j] == torch.tensor([1.,1.,1.]).to(DEVICE)).all():\n",
        "                        len2 += 1\n",
        "                        if 13+len2 == 25:\n",
        "                            break\n",
        "                    out_bar[i, len2-1, 2*j+1] = 1\n",
        "        return out_len, out_bar, out_col\n",
        "\n",
        "    def get_color(self, pixel):\n",
        "        \"\"\"\n",
        "        returns the encoded value for a pixel\n",
        "        \"\"\"\n",
        "        if (pixel == torch.tensor([0,0,0]).to(DEVICE)).all():  \n",
        "            return 0 # black\n",
        "        elif (pixel == torch.tensor([1,0,0]).to(DEVICE)).all():  \n",
        "            return 1 # red\n",
        "        elif (pixel == torch.tensor([0,0,1]).to(DEVICE)).all():  \n",
        "            return 2 # blue\n",
        "        elif (pixel == torch.tensor([0,1,0]).to(DEVICE)).all():  \n",
        "            return 3 # green\n",
        "        elif (pixel == torch.tensor([1,1,0]).to(DEVICE)).all():  \n",
        "            return 4 # yellow\n",
        "        else:\n",
        "            print(\"Something wrong!\")\n",
        "        \n",
        "                  \n",
        "\n",
        "    def classifier(self, x):\n",
        "        \"\"\"\n",
        "        classify an image (or a batch of images)\n",
        "        :param xs: a batch of scaled vectors of pixels from an image\n",
        "        :return: a batch of the corresponding class labels (as one-hots)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            zd_q_loc, zd_q_scale = self.qzd(x)\n",
        "            zd = zd_q_loc\n",
        "            alpha = F.softmax(self.qd(zd), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            d = x.new_zeros(alpha.size())\n",
        "            d = d.scatter_(1, ind, 1.0)\n",
        "\n",
        "            zy_q_loc, zy_q_scale = self.qzy.forward(x)\n",
        "            zy = zy_q_loc\n",
        "            alpha = F.softmax(self.qy(zy), dim=1)\n",
        "\n",
        "            # get the index (digit) that corresponds to\n",
        "            # the maximum predicted class probability\n",
        "            res, ind = torch.topk(alpha, 1)\n",
        "\n",
        "            # convert the digit(s) to one-hot tensor(s)\n",
        "            y = x.new_zeros(alpha.size())\n",
        "            y = y.scatter_(1, ind, 1.0)\n",
        "\n",
        "        return d, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "LdOsLfYJjBBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing model"
      ],
      "metadata": {
        "id": "R_H_mVMUszt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sJE0HTJE1ayP"
      },
      "outputs": [],
      "source": [
        "default_args = diva_args(zd_dim=32, zx_dim=64, zy_dim=32,\n",
        "                         aux_loss_multiplier_y=100, aux_loss_multiplier_d=2,\n",
        "                         rec_alpha = 50, rec_beta = 100, rec_gamma = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kxIMSeUD1949"
      },
      "outputs": [],
      "source": [
        "diva = StampDIVA(default_args).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(diva, [(1,45),(1,3,25,100),(1,2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9ZiBzfeZwro",
        "outputId": "77a19923-96c6-46fb-c69a-7058f1785374"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "StampDIVA                                --                        --\n",
              "├─qzd: 1-1                               [1, 32]                   --\n",
              "│    └─Sequential: 2-1                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-1                  [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-2                    [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-3               [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-5                    [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-6               [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-2                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-7                  [1, 32]                   135,200\n",
              "│    └─Sequential: 2-3                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-8                  [1, 32]                   135,200\n",
              "│    │    └─Softplus: 3-9                [1, 32]                   --\n",
              "├─qzx: 1-2                               [1, 64]                   --\n",
              "│    └─Sequential: 2-4                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-10                 [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-11                   [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-12              [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-13                 [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-14                   [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-15              [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-5                   [1, 64]                   --\n",
              "│    │    └─Linear: 3-16                 [1, 64]                   270,400\n",
              "│    └─Sequential: 2-6                   [1, 64]                   --\n",
              "│    │    └─Linear: 3-17                 [1, 64]                   270,400\n",
              "│    │    └─Softplus: 3-18               [1, 64]                   --\n",
              "├─qzy: 1-3                               [1, 32]                   --\n",
              "│    └─Sequential: 2-7                   [1, 64, 3, 22]            --\n",
              "│    │    └─Conv2d: 3-19                 [1, 32, 21, 96]           2,400\n",
              "│    │    └─ReLU: 3-20                   [1, 32, 21, 96]           --\n",
              "│    │    └─MaxPool2d: 3-21              [1, 32, 10, 48]           --\n",
              "│    │    └─Conv2d: 3-22                 [1, 64, 6, 44]            51,200\n",
              "│    │    └─ReLU: 3-23                   [1, 64, 6, 44]            --\n",
              "│    │    └─MaxPool2d: 3-24              [1, 64, 3, 22]            --\n",
              "│    └─Sequential: 2-8                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-25                 [1, 32]                   135,200\n",
              "│    └─Sequential: 2-9                   [1, 32]                   --\n",
              "│    │    └─Linear: 3-26                 [1, 32]                   135,200\n",
              "│    │    └─Softplus: 3-27               [1, 32]                   --\n",
              "├─px: 1-4                                [1, 100]                  --\n",
              "│    └─Sequential: 2-10                  [1, 200]                  --\n",
              "│    │    └─Linear: 3-28                 [1, 200]                  25,600\n",
              "│    │    └─ReLU: 3-29                   [1, 200]                  --\n",
              "│    └─Sequential: 2-11                  [1, 100]                  --\n",
              "│    │    └─Linear: 3-30                 [1, 100]                  20,100\n",
              "│    │    └─Softmax: 3-31                [1, 100]                  --\n",
              "│    └─Upsample: 2-12                    [1, 5, 200]               --\n",
              "│    └─Sequential: 2-13                  [1, 25, 200]              --\n",
              "│    │    └─ConvTranspose1d: 3-32        [1, 25, 200]              625\n",
              "│    │    └─ReLU: 3-33                   [1, 25, 200]              --\n",
              "│    └─Sequential: 2-14                  [1, 13, 200]              --\n",
              "│    │    └─Conv1d: 3-34                 [1, 13, 200]              988\n",
              "│    │    └─Softmax: 3-35                [1, 13, 200]              --\n",
              "│    └─Sequential: 2-15                  [1, 5, 200]               --\n",
              "│    │    └─Conv1d: 3-36                 [1, 5, 200]               380\n",
              "│    │    └─Softmax: 3-37                [1, 5, 200]               --\n",
              "├─pzd: 1-5                               [1, 32]                   --\n",
              "│    └─Sequential: 2-16                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-38                 [1, 32]                   1,440\n",
              "│    │    └─BatchNorm1d: 3-39            [1, 32]                   64\n",
              "│    │    └─ReLU: 3-40                   [1, 32]                   --\n",
              "│    └─Sequential: 2-17                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-41                 [1, 32]                   1,056\n",
              "│    └─Sequential: 2-18                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-42                 [1, 32]                   1,056\n",
              "│    │    └─Softplus: 3-43               [1, 32]                   --\n",
              "├─pzy: 1-6                               [1, 32]                   --\n",
              "│    └─Sequential: 2-19                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-44                 [1, 32]                   64\n",
              "│    │    └─BatchNorm1d: 3-45            [1, 32]                   64\n",
              "│    │    └─ReLU: 3-46                   [1, 32]                   --\n",
              "│    └─Sequential: 2-20                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-47                 [1, 32]                   1,056\n",
              "│    └─Sequential: 2-21                  [1, 32]                   --\n",
              "│    │    └─Linear: 3-48                 [1, 32]                   1,056\n",
              "│    │    └─Softplus: 3-49               [1, 32]                   --\n",
              "├─qd: 1-7                                [1, 45]                   --\n",
              "│    └─Linear: 2-22                      [1, 45]                   1,485\n",
              "├─qy: 1-8                                [1, 2]                    --\n",
              "│    └─Linear: 2-23                      [1, 2]                    66\n",
              "==========================================================================================\n",
              "Total params: 1,297,500\n",
              "Trainable params: 1,297,500\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 56.60\n",
              "==========================================================================================\n",
              "Input size (MB): 0.03\n",
              "Forward/backward pass size (MB): 2.03\n",
              "Params size (MB): 5.19\n",
              "Estimated Total Size (MB): 7.25\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diva.load_state_dict(torch.load(f'{link}/stampdiva_v3.0.1.pth'))"
      ],
      "metadata": {
        "id": "s5C6mSJTDLtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ce2227-5a26-4bcf-9c27-db8e6b6aa26d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "rH1E5J-ps3GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNA_dataset = MicroRNADataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myflmDPxjV40",
        "outputId": "bf637c82-d624-47f9-9e92-ae8bb13c1a15"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Labels! (~10s)\n",
            "Loading Names! (~5s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RNA_dataset_test = MicroRNADataset('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut2P5RSaMoDR",
        "outputId": "cbd88767-a68c-4acb-b03f-8b1496f3194e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Labels! (~10s)\n",
            "Loading Names! (~5s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ],
      "metadata": {
        "id": "YdYaqWvbjN26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eVGq463Y2m20"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(train_loader, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    epoch_class_y_loss = 0\n",
        "\n",
        "    no_batches = 0\n",
        "    pbar = tqdm(enumerate(train_loader), unit=\"batch\", \n",
        "                                     desc=f'Epoch {epoch}')\n",
        "    for batch_idx, (x, y, d) in pbar:\n",
        "        # To device\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        # print(d)\n",
        "        x, y, d = x.to(DEVICE), y.to(DEVICE), d.to(DEVICE)\n",
        "\n",
        "        # if (epoch % 50 == 0) and (batch_idx == 1):\n",
        "        #     save_reconstructions(model, d, x, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, class_y_loss = model.loss_function(d.float(), x.float(), y.float())\n",
        "        _, y_pred = model.classifier(x.float())\n",
        "        acc = ((y == y_pred).all(axis=1)*1.0).mean().item()\n",
        "        if writer is not None:\n",
        "          writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "          writer.add_scalar(\"y_loss/train\", class_y_loss, epoch)\n",
        "          writer.add_scalar(\"y_acc/train\", acc, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix(loss=loss.item()/x.shape[0], \n",
        "                         y_loss = class_y_loss.item()/x.shape[0])\n",
        "        train_loss += loss\n",
        "        epoch_class_y_loss += class_y_loss\n",
        "        no_batches += 1\n",
        "        # print(f'finished batch {no_batches}!')\n",
        "        # if no_batches == 25:\n",
        "        #     break\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    epoch_class_y_loss /= len(train_loader.dataset)\n",
        "\n",
        "    return train_loss, epoch_class_y_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_epoch(test_loader, model, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    epoch_class_y_loss = 0\n",
        "    test_corr = 0\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x,y,d) in enumerate(test_loader):\n",
        "            x, y, d = x.to(DEVICE), y.to(DEVICE), d.to(DEVICE)\n",
        "            loss, class_y_loss = model.loss_function(d.float(), x.float(), y.float())\n",
        "            _, y_pred = model.classifier(x.float())\n",
        "            test_corr += (y == y_pred).all(axis=1).sum().item()\n",
        "            acc = ((y == y_pred).all(axis=1)*1.).mean().item()\n",
        "            if writer is not None:\n",
        "              writer.add_scalar(\"Loss/test\", loss, epoch)\n",
        "              writer.add_scalar(\"y_loss/test\", class_y_loss, epoch)\n",
        "              writer.add_scalar(\"y_acc/test\", acc, epoch)\n",
        "            test_loss += loss\n",
        "            epoch_class_y_loss += class_y_loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    epoch_class_y_loss /= len(test_loader.dataset)\n",
        "    acc = test_corr/len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, epoch_class_y_loss, acc\n",
        "  "
      ],
      "metadata": {
        "id": "dT7E0C3nM3qh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, diva, optimizer, end_epoch, start_epoch=0):\n",
        "    epoch_loss_sup = []\n",
        "    epoch_loss_y = []\n",
        "\n",
        "    y_loss_test = []\n",
        "    test_loss = []\n",
        "    test_acc_lst = []\n",
        "\n",
        "    for epoch in range(start_epoch+1, end_epoch+1):\n",
        "        avg_epoch_losses_sup, avg_epoch_class_y_loss = train_single_epoch(train_loader, diva, optimizer, epoch)\n",
        "        str_loss_sup = avg_epoch_losses_sup\n",
        "        epoch_loss_sup.append(avg_epoch_losses_sup)\n",
        "        epoch_loss_y.append(avg_epoch_class_y_loss)\n",
        "        str_print = \"epoch {}: avg train loss {}\".format(epoch, str_loss_sup)\n",
        "        str_print += \", class y train loss {}\".format(avg_epoch_class_y_loss)\n",
        "        print(str_print)\n",
        "\n",
        "        test_lss, epoch_class_y_loss_test, test_acc = test_single_epoch(test_loader, diva, epoch)\n",
        "        test_loss.append(test_lss)\n",
        "        y_loss_test.append(epoch_class_y_loss_test)\n",
        "        test_acc_lst.append(test_acc)\n",
        "        str_print = \"epoch {}: avg test loss {}\".format(epoch, test_lss)\n",
        "        str_print += \", class y test loss {}\".format(epoch_class_y_loss_test)\n",
        "        str_print += \", test accuracy {}\".format(test_acc)\n",
        "        print(str_print)\n",
        "    if writer is not None:    \n",
        "      writer.flush()\n",
        "    return epoch_loss_sup, epoch_loss_y"
      ],
      "metadata": {
        "id": "npLjVGs0jHYn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "nI4-NzHxjmci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "48B39rFl79Yh"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(RNA_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(RNA_dataset_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "J6y2Ek2677z1"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(diva.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0m47XoL87oLs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "da8b01e3-4239-427a-c32c-3476d1adc2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5: 0batch [00:00, ?batch/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "Epoch 5: 272batch [1:23:15, 18.37s/batch, loss=4.59e+4, y_loss=0.469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5: avg train loss 41880.984375, class y train loss 0.5080251693725586\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c1e239995c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meplss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-85bc13bcae92>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, diva, optimizer, end_epoch, start_epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_print\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtest_lss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_class_y_loss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_lss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_loss_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_class_y_loss_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c35c390f2173>\u001b[0m in \u001b[0;36mtest_single_epoch\u001b[0;34m(test_loader, model, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_y_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtest_corr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8b2b800f251f>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, d, x, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m           \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqzd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpzd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzd_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqzx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpzx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzx_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqzy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpzy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzy_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m           \u001b[0mout_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoded_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           \u001b[0mCE_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8b2b800f251f>\u001b[0m in \u001b[0;36mget_encoded_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlen1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;31m# loop until white pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                         \u001b[0mlen1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lss, eplss = train(train_loader, diva, optimizer, 6, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "14_0zzMCtR7Y"
      },
      "outputs": [],
      "source": [
        "torch.save(diva.state_dict(), f'{link}/stampdiva_v3.0.2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JUqEE7bHGjO"
      },
      "outputs": [],
      "source": [
        "lss, eplss = train(train_loader, diva, optimizer, 8, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYgruvcHGjP"
      },
      "outputs": [],
      "source": [
        "torch.save(diva.state_dict(), f'{link}/stampdiva_v3.0.3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gl6VY6zFHDg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "whsgNltzXDhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling from trained model"
      ],
      "metadata": {
        "id": "VfcwhSNIjqIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "blYW4SMZG4fv"
      },
      "outputs": [],
      "source": [
        "a = next(enumerate(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "deiEYKxlmphu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1c0a7a-e25c-4b17-cab8-55d5c8688c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    diva.eval()\n",
        "    d = a[1][2][:9].to(DEVICE).float()\n",
        "    x = a[1][0][:9].to(DEVICE).float()\n",
        "    y = a[1][1][:9].to(DEVICE).float()\n",
        "    x_1, x_2, x_3, _, _, _, _, _, _, _, _, _, _, _ = diva(d,x,y)\n",
        "    out = diva.px.reconstruct_image(x_1, x_2, x_3)\n",
        "    #sample_x = sample(x_recon.cpu().numpy(), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4NWGV4Xd7bn8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "817621b8-7e77-41fb-9011-a9f97515df37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3600x3600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(50,50))\n",
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(out[i])\n",
        "plt.savefig('divastamprecon.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OE3qVVFFLaPm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "906c9568-cd6e-4a8c-c224-5ffd0caac4e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
        "for i in range(9):\n",
        "  ax[i//3, i%3].imshow(x[i].cpu().permute(1,2,0))\n",
        "  \n",
        "plt.savefig('divastamporg.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RRQU05xQEx28"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "StampDIVA",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}